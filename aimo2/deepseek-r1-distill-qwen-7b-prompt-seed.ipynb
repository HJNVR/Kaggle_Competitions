{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c5f1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:25:53.319311Z",
     "iopub.status.busy": "2025-03-11T15:25:53.319005Z",
     "iopub.status.idle": "2025-03-11T15:27:03.730393Z",
     "shell.execute_reply": "2025-03-11T15:27:03.729671Z"
    },
    "papermill": {
     "duration": 70.417985,
     "end_time": "2025-03-11T15:27:03.731689",
     "exception": false,
     "start_time": "2025-03-11T15:25:53.313704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 15:26:57 __init__.py:183] Automatically detected platform cuda.\n",
      "PyTorch version: 2.5.1+cu124\n",
      "vLLM: 0.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('vLLM:', vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2323bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:27:03.740937Z",
     "iopub.status.busy": "2025-03-11T15:27:03.740697Z",
     "iopub.status.idle": "2025-03-11T15:27:03.755979Z",
     "shell.execute_reply": "2025-03-11T15:27:03.755379Z"
    },
    "papermill": {
     "duration": 0.021071,
     "end_time": "2025-03-11T15:27:03.757177",
     "exception": false,
     "start_time": "2025-03-11T15:27:03.736106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=0)\n",
    "\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 45) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6ea3ac",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T15:27:03.765834Z",
     "iopub.status.busy": "2025-03-11T15:27:03.765615Z",
     "iopub.status.idle": "2025-03-11T15:28:59.800533Z",
     "shell.execute_reply": "2025-03-11T15:28:59.799678Z"
    },
    "papermill": {
     "duration": 116.040979,
     "end_time": "2025-03-11T15:28:59.802198",
     "exception": false,
     "start_time": "2025-03-11T15:27:03.761219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 15:27:33 config.py:526] This model supports multiple tasks: {'reward', 'score', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-11 15:27:37 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 03-11 15:27:37 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 03-11 15:27:37 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 15:27:37 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1', speculative_config=None, tokenizer='/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=12288, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[32,24,16,8,4,2,1],\"max_capture_size\":32}, use_cached_outputs=False, \n",
      "WARNING 03-11 15:27:38 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-11 15:27:38 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:27:38 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:27:38 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:27:38 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "WARNING 03-11 15:27:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-11 15:27:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:27:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:27:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:27:39 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-11 15:27:39 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-11 15:27:39 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:27:39 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-11 15:27:50 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-11 15:27:50 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:27:50 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-11 15:27:50 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:27:50 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 03-11 15:27:50 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:27:50 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:27:50 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 03-11 15:27:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 15:27:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-11 15:27:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-11 15:27:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 03-11 15:27:51 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_62e6d0d9'), local_subscribe_port=47111, remote_subscribe_port=None)\n",
      "INFO 03-11 15:27:51 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:27:51 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:27:51 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "INFO 03-11 15:27:51 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f5d4fe3fea42e5ac8dad9ec5360944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:28:25 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "INFO 03-11 15:28:25 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:28:25 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:28:25 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 15:28:44 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:44 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 15:28:44 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-11 15:28:44 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 03-11 15:28:44 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] Memory profiling takes 18.64 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] Memory profiling takes 18.64 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 03-11 15:28:44 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] Memory profiling takes 18.64 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "INFO 03-11 15:28:44 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:28:44 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 15:28:45 worker.py:266] Memory profiling takes 18.97 seconds\r\n",
      "INFO 03-11 15:28:45 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-11 15:28:45 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "INFO 03-11 15:28:45 executor_base.py:108] # CUDA blocks: 88385, # CPU blocks: 18724\n",
      "INFO 03-11 15:28:45 executor_base.py:113] Maximum concurrency for 12288 tokens per request: 115.08x\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 15:28:45 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 15:28:50 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-11 15:28:50 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-11 15:28:50 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-11 15:28:50 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  86%|████████▌ | 6/7 [00:05<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-11 15:28:57 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n",
      "INFO 03-11 15:28:57 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 15:28:57 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 7/7 [00:07<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 15:28:57 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n",
      "INFO 03-11 15:28:57 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 31.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\n",
    "else:\n",
    "    llm_model_pth = '/root/volume/KirillR/QwQ-32B-Preview-AWQ'\n",
    "\n",
    "MAX_NUM_SEQS = 32 #16\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2 #16384 #32768 #\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "#    dtype=\"half\",                 # The data type for the model weights and activations\n",
    "    max_num_seqs=MAX_NUM_SEQS,    # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN,  # Model context length\n",
    "    trust_remote_code=True,       # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,       # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2024,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a8b96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:28:59.825892Z",
     "iopub.status.busy": "2025-03-11T15:28:59.825606Z",
     "iopub.status.idle": "2025-03-11T15:28:59.835103Z",
     "shell.execute_reply": "2025-03-11T15:28:59.834503Z"
    },
    "papermill": {
     "duration": 0.02202,
     "end_time": "2025-03-11T15:28:59.836085",
     "exception": false,
     "start_time": "2025-03-11T15:28:59.814065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers\n",
    "\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    max_tokens = MAX_MODEL_LEN\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        print(\"Speedrun\")\n",
    "        max_tokens = 2 * MAX_MODEL_LEN // 3\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.6,               # Randomness of the sampling\n",
    "        # top_p=0.90,                    # Cumulative probability of the top tokens to consider\n",
    "        # min_p=0.05,                    # Minimum probability for a token to be considered\n",
    "        skip_special_tokens=True,      # Whether to skip special tokens in the output\n",
    "        max_tokens=max_tokens,         # Maximum number of tokens to generate\n",
    "        stop=[\"</think>\"],             # List of strings that stop the generation\n",
    "        seed=2025,\n",
    "    )\n",
    "    \n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    print([len(single_request_output.outputs[0].token_ids) for single_request_output in request_output])\n",
    "\n",
    "    sort_keys_and_list_of_messages = []\n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        #print()\n",
    "        #print(single_request_output.outputs[0].text)\n",
    "        #print()\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "        sort_keys_and_list_of_messages.append(\n",
    "            (\n",
    "                len(single_request_output.outputs[0].token_ids),\n",
    "                messages\n",
    "            )\n",
    "        )\n",
    "    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n",
    "    sort_keys_and_list_of_messages.sort(key=lambda sort_key_and_messages: sort_key_and_messages[0])\n",
    "    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n",
    "    \n",
    "    list_of_messages = [messages for _, messages in sort_keys_and_list_of_messages]\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878c5911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:28:59.859471Z",
     "iopub.status.busy": "2025-03-11T15:28:59.859200Z",
     "iopub.status.idle": "2025-03-11T15:28:59.862925Z",
     "shell.execute_reply": "2025-03-11T15:28:59.862292Z"
    },
    "papermill": {
     "duration": 0.016141,
     "end_time": "2025-03-11T15:28:59.864022",
     "exception": false,
     "start_time": "2025-03-11T15:28:59.847881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correct_answer(question):\n",
    "    if 'Three airline' in question: return 79\n",
    "    if 'Fred and George' in question: return 250\n",
    "    if 'Triangle $ABC$' in question: return 180\n",
    "    if 'Find the three' in question: return 143\n",
    "    if 'We call a' in question: return 3\n",
    "    if 'Let $ABC$ be' in question: return 751\n",
    "    if 'For a positive' in question: return 891\n",
    "    if 'For positive integers' in question: return 810\n",
    "    if 'The Fibonacci numbers' in question: return 201\n",
    "    if 'Alice writes all' in question: return 902\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0695a2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:28:59.886888Z",
     "iopub.status.busy": "2025-03-11T15:28:59.886648Z",
     "iopub.status.idle": "2025-03-11T15:28:59.898627Z",
     "shell.execute_reply": "2025-03-11T15:28:59.898027Z"
    },
    "papermill": {
     "duration": 0.024951,
     "end_time": "2025-03-11T15:28:59.899630",
     "exception": false,
     "start_time": "2025-03-11T15:28:59.874679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    options = []\n",
    "    # for _ in range(13):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"},\n",
    "    #             {\"role\": \"user\", \"content\": question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(3):    \n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}.\"},\n",
    "    #             {\"role\": \"user\", \"content\": question},\n",
    "    #         ],\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"Please solve the problems with deep resoning. You are careful and always recheck your conduction. You will never give answer directly until you have enough confidence. You should think step-by-step. \\\n",
    "    #                                         Return final answer within \\\\boxed{}, after taking modulo 1000. \\n\" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"Please approach problems through a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracking, and iteration, fostering a thoughtful and well-considered problem-solving process. \\\n",
    "    #                                         Put your final answer in \\\\boxed{}. If the answer exceeds 1000, take the answer modulo 1000.\\n\" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"Break the problem into sub-steps, reasoning at each stage, and give the final answer in the format \\\\boxed{}. If the answer exceeds 1000, take the answer modulo 1000. \\n\" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"You should think step-by-step and you are good at reverse thinking to recheck your answer and fix all possible mistakes. After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}. \" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"Please carefully read the problem statement first to ensure you fully understand its meaning and key points. Then, solve the problem correctly and completely through deep reasoning. Finally, return the result modulo 1000 and enclose it in \\\\boxed{}.\" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"Break the problem into sub-steps, reasoning at each stage, and give the final answer in the format \\\\boxed{}. If the answer exceeds 1000, take the answer modulo 1000. \\n\" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    # for _ in range(2):\n",
    "    #     options.append(\n",
    "    #         [\n",
    "    #             {\"role\": \"user\", \"content\": \"Please approach problems through a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracking, and iteration, fostering a thoughtful and well-considered problem-solving process. \\\n",
    "    #                                         Put your final answer in \\\\boxed{}. If the answer exceeds 1000, take the answer modulo 1000.\\n\" + question},\n",
    "    #         ]\n",
    "    #     )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Process the problem through isomorphic transformation:\n",
    "a) Establish equivalent representations in different mathematical domains\n",
    "b) Solve in the most advantageous domain using vertical depth-first search\n",
    "c) Map solution back to original problem space\n",
    "d) Validate isomorphism preservation through bidirectional mapping\n",
    "e) Execute convergence testing on numerical results\n",
    "If the final answer exceeds 1000, take the final answer modulo 1000.\n",
    "Final answer (0-999) in \\\\boxed{} after cross-domain confirmation. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Employ operational stack methodology:\n",
    "1. Layer 1: Pattern recognition and theorem mapping\n",
    "2. Layer 2: Lemma decomposition and dependency resolution\n",
    "3. Layer 3: Atomic operation execution with rollback capability\n",
    "4. Integrity check: Proof-tree validation\n",
    "5. Optimize: Perform minimal sufficient computation proof\n",
    "Return \\\\boxed{answer} with value mod 1000 after stack-wide verification. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Analyze this problem through first-principles reasoning.\n",
    "1) Decompose into core components\n",
    "2) Identify relevant theorems/strategies\n",
    "3) Formulate key conjectures with verification checkpoints\n",
    "4) Execute solution with cross-validation at critical steps\n",
    "5) Perform boundary case analysis\n",
    "6) Document solution path concisely\n",
    "After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}. Prioritize quality over explanation length. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Solve this IMO-level problem using strategic phase transitions:\n",
    "Phase 1: Problem comprehension and pattern mapping (1 mental cycle)\n",
    "Phase 2: Dimensional reduction through invariant identification\n",
    "Phase 3: Backward-forward search with heuristic pruning\n",
    "Phase 4: Convergence verification through dual methods verification\n",
    "Return only the final verified answer modulo 1000: \\\\boxed{}. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Execute these steps strictly:\n",
    "1. Cryptographic scan of problem statement\n",
    "2. Multivariate analysis with combinatorial sanitation\n",
    "3. Apply hybrid methods (analytic + algebraic topology where appropriate)\n",
    "4. Error detection via inverse problem reconstruction\n",
    "5. Precision result formatting\n",
    "If the final answer exceeds 1000, take the final answer modulo 1000.\n",
    "Final numeric answer (0-999): \\\\boxed{}. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Address this problem through constraints decomposition:\n",
    "- Isolate key variables and their relationships\n",
    "- Develop solution pathways for distinct constraint subsets\n",
    "- Synthesize partial solutions using combinatorial logic\n",
    "- Perform dimensional analysis on numerical results\n",
    "- Apply sanity checks through limit case evaluation\n",
    "Return final answer modulo 1000 in \\\\boxed{} after comprehensive verification. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"\"As an expert mathematical problem solver, systematically analyze the problem through these stages:\n",
    "1. Problem Breakdown: Identify core components and hidden constraints\n",
    "2. Strategy Formulation: Propose multiple approaches with theoretical justification\n",
    "3. Tactical Execution: Implement chosen method with rigorous logical deductions\n",
    "4. Critical Verification: Perform sanity checks and alternative path validation\n",
    "5. Precision Refinement: Ensure technical accuracy at every computational step\n",
    "Present modular reasoning at each stage. Finalize by reporting the final answer modulo 1000 within \\\\boxed{}. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": \"\"\"Solve this mathematical problem using deep theoretical analysis:\n",
    "- Frame the problem within relevant mathematical domains\n",
    "- Identify applicable theorems/lemmas with explicit citations\n",
    "- Develop formal proofs for non-trivial steps\n",
    "- Examine edge cases and boundary conditions\n",
    "- Quantitatively verify dimensional consistency\n",
    "Execute reverse-engineering verification before finalizing. Present conclusion modulo 1000 in \\\\boxed{}. \\n\n",
    "\"\"\" + question},\n",
    "            ]\n",
    "        )\n",
    "    return options[index%len(options)]\n",
    "\n",
    "g_score = 0\n",
    "g_count = 0\n",
    "def predict_for_question(question: str) -> int:\n",
    "    # selected_questions_only = True\n",
    "    # #selected_questions_only = False\n",
    "    # if selected_questions_only and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    #     #if \"Triangle\" not in question:\n",
    "    #     #    return 210\n",
    "    #     if \"Triangle\" not in question and \"delightful\" not in question and \"George\" not in question:\n",
    "    #         return 210\n",
    "\n",
    "    global g_score\n",
    "    global g_count\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "    \n",
    "    print(question)\n",
    "\n",
    "    num_seqs = MAX_NUM_SEQS\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        num_seqs = 2 * MAX_NUM_SEQS // 3\n",
    "    \n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(num_seqs)]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    for _ in range(1):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        \n",
    "        if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"question\": [question] * len(list_of_messages),\n",
    "                    \"message\": [messages[-1][\"content\"] for messages in list_of_messages],\n",
    "                }\n",
    "            )\n",
    "            df.to_csv(f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False)\n",
    "        \n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "    \n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    correct_answer = get_correct_answer(question)\n",
    "    print(answer)\n",
    "    if str(answer) == str(correct_answer):\n",
    "        g_score += 1\n",
    "    g_count += 1\n",
    "    print(\"\\n\\n\")\n",
    "    cutoff_times.pop()\n",
    "    print(f\"score: {g_score}/{g_count}\")\n",
    "    return answer\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafbc472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:28:59.921719Z",
     "iopub.status.busy": "2025-03-11T15:28:59.921502Z",
     "iopub.status.idle": "2025-03-11T15:28:59.924150Z",
     "shell.execute_reply": "2025-03-11T15:28:59.923560Z"
    },
    "papermill": {
     "duration": 0.014699,
     "end_time": "2025-03-11T15:28:59.925153",
     "exception": false,
     "start_time": "2025-03-11T15:28:59.910454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#predict_for_question(\"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b272976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:28:59.946846Z",
     "iopub.status.busy": "2025-03-11T15:28:59.946621Z",
     "iopub.status.idle": "2025-03-11T15:28:59.983028Z",
     "shell.execute_reply": "2025-03-11T15:28:59.982429Z"
    },
    "papermill": {
     "duration": 0.048393,
     "end_time": "2025-03-11T15:28:59.984139",
     "exception": false,
     "start_time": "2025-03-11T15:28:59.935746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86043537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:29:00.005880Z",
     "iopub.status.busy": "2025-03-11T15:29:00.005663Z",
     "iopub.status.idle": "2025-03-11T16:25:58.555753Z",
     "shell.execute_reply": "2025-03-11T16:25:58.554842Z"
    },
    "papermill": {
     "duration": 3418.562419,
     "end_time": "2025-03-11T16:25:58.557131",
     "exception": false,
     "start_time": "2025-03-11T15:28:59.994712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [05:41<00:00, 10.66s/it, est. speed input: 14.27 toks/s, output: 868.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12122, 12122, 12144, 5900, 12126, 7104, 5049, 11042, 8529, 8511, 10047, 10733, 11188, 12119, 12145, 9985, 12122, 12122, 5359, 5875, 4778, 10847, 12149, 8333, 12135, 9417, 5680, 7929, 5275, 8724, 8615, 6101]\n",
      "[12122, 12122, 12144, 5900, 12126, 7104, 5049, 11042, 8529, 8511, 10047, 10733, 11188, 12119, 12145, 9985, 12122, 12122, 5359, 5875, 4778, 10847, 12149, 8333, 12135, 9417, 5680, 7929, 5275, 8724, 8615, 6101]\n",
      "[4778, 5049, 5275, 5359, 5680, 5875, 5900, 6101, 7104, 7929, 8333, 8511, 8529, 8615, 8724, 9417, 9985, 10047, 10733, 10847, 11042, 11188, 12119, 12122, 12122, 12122, 12122, 12126, 12135, 12144, 12145, 12149]\n",
      "['19', '917', '891', '891', '892', '891', '892', '891', '891', '891', '891', '26', '18', '891', '900', '35', '18', '18', '908', '900', '873', '899']\n",
      "891\n",
      "\n",
      "\n",
      "\n",
      "score: 1/1\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [03:37<00:00,  6.80s/it, est. speed input: 32.23 toks/s, output: 800.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6508, 5320, 6530, 5634, 5408, 4300, 5673, 6260, 6069, 4154, 4003, 5202, 2660, 5009, 3673, 4094, 5178, 6008, 6164, 6333, 5347, 3442, 5627, 4916, 5033, 5377, 5894, 5399, 5222, 5281, 10008, 8347]\n",
      "[6508, 5320, 6530, 5634, 5408, 4300, 5673, 6260, 6069, 4154, 4003, 5202, 2660, 5009, 3673, 4094, 5178, 6008, 6164, 6333, 5347, 3442, 5627, 4916, 5033, 5377, 5894, 5399, 5222, 5281, 10008, 8347]\n",
      "[2660, 3442, 3673, 4003, 4094, 4154, 4300, 4916, 5009, 5033, 5178, 5202, 5222, 5281, 5320, 5347, 5377, 5399, 5408, 5627, 5634, 5673, 5894, 6008, 6069, 6164, 6260, 6333, 6508, 6530, 8347, 10008]\n",
      "['201', '201', '201', '201', '201', '201', '201', '201', '201', '201', '0', '200', '201', '201', '200', '202', '201', '201', '201', '201', '201', '202', '201', '201', '201', '200', '201', '201', '201', '201', '0', '201']\n",
      "201\n",
      "\n",
      "\n",
      "\n",
      "score: 2/2\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [06:24<00:00, 12.02s/it, est. speed input: 15.32 toks/s, output: 950.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11874, 12090, 12112, 12112, 12094, 8104, 12117, 10117, 11259, 9192, 12123, 12123, 12087, 12087, 12113, 12113, 10032, 12090, 7826, 12112, 12094, 12094, 12117, 12117, 12103, 12103, 12123, 9696, 12087, 8920, 12113, 12113]\n",
      "[11874, 12090, 12112, 12112, 12094, 8104, 12117, 10117, 11259, 9192, 12123, 12123, 12087, 12087, 12113, 12113, 10032, 12090, 7826, 12112, 12094, 12094, 12117, 12117, 12103, 12103, 12123, 9696, 12087, 8920, 12113, 12113]\n",
      "[7826, 8104, 8920, 9192, 9696, 10032, 10117, 11259, 11874, 12087, 12087, 12087, 12090, 12090, 12094, 12094, 12094, 12103, 12103, 12112, 12112, 12112, 12113, 12113, 12113, 12113, 12117, 12117, 12117, 12123, 12123, 12123]\n",
      "['40', '40', '100', '588', '40', '55', '40', '150', '99']\n",
      "40\n",
      "\n",
      "\n",
      "\n",
      "score: 2/3\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [06:09<00:00, 11.55s/it, est. speed input: 15.60 toks/s, output: 946.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11300, 12094, 12116, 12116, 9829, 12098, 12121, 11251, 12107, 8903, 12127, 12127, 12091, 12091, 7725, 11767, 9635, 11343, 9857, 12116, 11491, 12098, 7470, 12121, 12107, 12107, 8111, 7179, 12091, 11655, 6792, 11538]\n",
      "[11300, 12094, 12116, 12116, 9829, 12098, 12121, 11251, 12107, 8903, 12127, 12127, 12091, 12091, 7725, 11767, 9635, 11343, 9857, 12116, 11491, 12098, 7470, 12121, 12107, 12107, 8111, 7179, 12091, 11655, 6792, 11538]\n",
      "[6792, 7179, 7470, 7725, 8111, 8903, 9635, 9829, 9857, 11251, 11300, 11343, 11491, 11538, 11655, 11767, 12091, 12091, 12091, 12094, 12098, 12098, 12107, 12107, 12107, 12116, 12116, 12116, 12121, 12121, 12127, 12127]\n",
      "['250', '750', '500', '250', '875', '0', '250', '0', '750', '250', '250', '000', '750', '0', '375', '0', 'answer']\n",
      "250\n",
      "\n",
      "\n",
      "\n",
      "score: 3/4\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [06:44<00:00, 12.64s/it, est. speed input: 15.76 toks/s, output: 944.17 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12075, 12075, 12097, 12097, 12079, 12079, 12102, 11732, 12088, 9600, 12108, 12108, 12072, 12072, 12098, 12098, 12075, 12075, 12097, 12097, 12079, 12079, 11737, 12102, 12088, 12088, 12108, 12108, 12072, 12072, 10837, 11511]\n",
      "[12075, 12075, 12097, 12097, 12079, 12079, 12102, 11732, 12088, 9600, 12108, 12108, 12072, 12072, 12098, 12098, 12075, 12075, 12097, 12097, 12079, 12079, 11737, 12102, 12088, 12088, 12108, 12108, 12072, 12072, 10837, 11511]\n",
      "[9600, 10837, 11511, 11732, 11737, 12072, 12072, 12072, 12072, 12075, 12075, 12075, 12075, 12079, 12079, 12079, 12079, 12088, 12088, 12088, 12097, 12097, 12097, 12097, 12098, 12098, 12102, 12102, 12108, 12108, 12108, 12108]\n",
      "['2', '0', '0', '1000', '1000', 'answer']\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "score: 3/5\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [05:01<00:00,  9.41s/it, est. speed input: 16.05 toks/s, output: 731.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4543, 7844, 5381, 5803, 12127, 7127, 4383, 6917, 5362, 6114, 6706, 7050, 3840, 5523, 12146, 8404, 7470, 5259, 6586, 4752, 5950, 5208, 6583, 4516, 6175, 12136, 9471, 6019, 6174, 6903, 10628, 7399]\n",
      "[4543, 7844, 5381, 5803, 12127, 7127, 4383, 6917, 5362, 6114, 6706, 7050, 3840, 5523, 12146, 8404, 7470, 5259, 6586, 4752, 5950, 5208, 6583, 4516, 6175, 12136, 9471, 6019, 6174, 6903, 10628, 7399]\n",
      "[3840, 4383, 4516, 4543, 4752, 5208, 5259, 5362, 5381, 5523, 5803, 5950, 6019, 6114, 6174, 6175, 6583, 6586, 6706, 6903, 6917, 7050, 7127, 7399, 7470, 7844, 8404, 9471, 10628, 12127, 12136, 12146]\n",
      "['180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180']\n",
      "180\n",
      "\n",
      "\n",
      "\n",
      "score: 4/6\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [05:38<00:00, 10.58s/it, est. speed input: 16.74 toks/s, output: 844.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12097, 12097, 12119, 5833, 3312, 12101, 5882, 5037, 12110, 6931, 12130, 12130, 12094, 7962, 4307, 12120, 9758, 12097, 5406, 12119, 8217, 5405, 7858, 12124, 12110, 6554, 4148, 5886, 12094, 3869, 12120, 7784]\n",
      "[12097, 12097, 12119, 5833, 3312, 12101, 5882, 5037, 12110, 6931, 12130, 12130, 12094, 7962, 4307, 12120, 9758, 12097, 5406, 12119, 8217, 5405, 7858, 12124, 12110, 6554, 4148, 5886, 12094, 3869, 12120, 7784]\n",
      "[3312, 3869, 4148, 4307, 5037, 5405, 5406, 5833, 5882, 5886, 6554, 6931, 7784, 7858, 7962, 8217, 9758, 12094, 12094, 12097, 12097, 12097, 12101, 12110, 12110, 12119, 12119, 12120, 12120, 12124, 12130, 12130]\n",
      "['902', '902', '902', '902', '902', '902', '902', '902', '902', '902', '564', '488', '902', '902', '902', '902', '902']\n",
      "902\n",
      "\n",
      "\n",
      "\n",
      "score: 5/7\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [06:33<00:00, 12.30s/it, est. speed input: 19.43 toks/s, output: 954.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12035, 12035, 12057, 12057, 12039, 12039, 12062, 12062, 7360, 12048, 12068, 12068, 12032, 12032, 12058, 12058, 12035, 12035, 12057, 12057, 12039, 12039, 12062, 12062, 12048, 7085, 12068, 12068, 12032, 12032, 12058, 12058]\n",
      "[12035, 12035, 12057, 12057, 12039, 12039, 12062, 12062, 7360, 12048, 12068, 12068, 12032, 12032, 12058, 12058, 12035, 12035, 12057, 12057, 12039, 12039, 12062, 12062, 12048, 7085, 12068, 12068, 12032, 12032, 12058, 12058]\n",
      "[7085, 7360, 12032, 12032, 12032, 12032, 12035, 12035, 12035, 12035, 12039, 12039, 12039, 12039, 12048, 12048, 12057, 12057, 12057, 12057, 12058, 12058, 12058, 12058, 12062, 12062, 12062, 12062, 12068, 12068, 12068, 12068]\n",
      "['0', '0']\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "score: 5/8\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [04:35<00:00,  8.60s/it, est. speed input: 17.22 toks/s, output: 630.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4386, 7221, 4407, 4767, 7601, 5052, 5691, 4142, 3347, 5169, 3654, 6708, 3420, 4038, 7282, 5195, 4326, 8030, 3094, 3792, 4887, 5485, 5730, 4622, 3122, 4567, 4856, 7416, 5746, 7506, 6035, 12149]\n",
      "[4386, 7221, 4407, 4767, 7601, 5052, 5691, 4142, 3347, 5169, 3654, 6708, 3420, 4038, 7282, 5195, 4326, 8030, 3094, 3792, 4887, 5485, 5730, 4622, 3122, 4567, 4856, 7416, 5746, 7506, 6035, 12149]\n",
      "[3094, 3122, 3347, 3420, 3654, 3792, 4038, 4142, 4326, 4386, 4407, 4567, 4622, 4767, 4856, 4887, 5052, 5169, 5195, 5485, 5691, 5730, 5746, 6035, 6708, 7221, 7282, 7416, 7506, 7601, 8030, 12149]\n",
      "['143', '143', '143', '143', '101', '143', '143', '101', '143', '143', '143', '143', '143', '143', '143', '143', '143', '143', '143', '143', '143', '101', '143', '143', '143', '143', '143', '143', '143', '143', '143']\n",
      "143\n",
      "\n",
      "\n",
      "\n",
      "score: 6/9\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [06:31<00:00, 12.24s/it, est. speed input: 19.13 toks/s, output: 985.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12040, 12040, 12062, 12062, 12044, 12044, 12067, 12067, 12053, 12053, 12073, 12073, 12037, 12037, 12063, 12063, 12040, 12040, 12062, 12062, 12044, 12044, 12067, 12067, 12053, 12053, 12073, 12073, 12037, 12037, 12063, 12063]\n",
      "[12040, 12040, 12062, 12062, 12044, 12044, 12067, 12067, 12053, 12053, 12073, 12073, 12037, 12037, 12063, 12063, 12040, 12040, 12062, 12062, 12044, 12044, 12067, 12067, 12053, 12053, 12073, 12073, 12037, 12037, 12063, 12063]\n",
      "[12037, 12037, 12037, 12037, 12040, 12040, 12040, 12040, 12044, 12044, 12044, 12044, 12053, 12053, 12053, 12053, 12062, 12062, 12062, 12062, 12063, 12063, 12063, 12063, 12067, 12067, 12067, 12067, 12073, 12073, 12073, 12073]\n",
      "[]\n",
      "210\n",
      "\n",
      "\n",
      "\n",
      "score: 6/10\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "#            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a3b75",
   "metadata": {
    "papermill": {
     "duration": 0.022619,
     "end_time": "2025-03-11T16:25:58.603513",
     "exception": false,
     "start_time": "2025-03-11T16:25:58.580894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 220483900,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 224053,
     "modelInstanceId": 206829,
     "sourceId": 242129,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3615.137546,
   "end_time": "2025-03-11T16:26:04.357363",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T15:25:49.219817",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "102320f372f34dd483e0d95060cf7885": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1617e870adcb46a3b2e7a3cbcf83aad2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "22f5d4fe3fea42e5ac8dad9ec5360944": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e3b8f35e58ea4769812c9cc7640d8ce3",
        "IPY_MODEL_dca141493a324a539b6064be42878b8e",
        "IPY_MODEL_a79951842a7f4af1a9a1f1e86f69f6f0"
       ],
       "layout": "IPY_MODEL_fdd176962ac04fd28c2fffcc5999c6e5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "50c5f41890564c5494c99cf739cf32cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a79951842a7f4af1a9a1f1e86f69f6f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50c5f41890564c5494c99cf739cf32cb",
       "placeholder": "​",
       "style": "IPY_MODEL_ae3d0202d2f244a8b2df57e4e0230ebb",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:33&lt;00:00, 14.21s/it]\n"
      }
     },
     "ae3d0202d2f244a8b2df57e4e0230ebb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb9472386f454fea985e0206a00878f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dca141493a324a539b6064be42878b8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fca9dcf279934c509de8b7c58211d0d0",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1617e870adcb46a3b2e7a3cbcf83aad2",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "e3b8f35e58ea4769812c9cc7640d8ce3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_102320f372f34dd483e0d95060cf7885",
       "placeholder": "​",
       "style": "IPY_MODEL_bb9472386f454fea985e0206a00878f9",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "fca9dcf279934c509de8b7c58211d0d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fdd176962ac04fd28c2fffcc5999c6e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
